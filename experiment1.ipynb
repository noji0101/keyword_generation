{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryogo/.local/share/virtualenvs/keyword_generation-2Tlv01LJ/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to /Users/ryogo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ryogo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from word_extractor.lexemes_vector import get_most_similar_lexemes, get_verctor_from_word, get_verctor_from_sense_key, get_word_from_vector, get_analogy\n",
    "from word_extractor.wordnet import get_hypernum, get_hyponym, get_not_similar_hyponym\n",
    "from utils.dataloader import read_lexemes_dict_and_list, read_mapping\n",
    "\n",
    "input_words = ['car', 'toughness', 'velocity', 'tire']\n",
    "synset_filepath = './data/synsets.txt'\n",
    "lexemes_filepath = './data/lexemes.txt'\n",
    "mapping_filepath = './data/mapping.txt'\n",
    "word2vec_filepath = './data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "lexemes_dict, lexemes_list = read_lexemes_dict_and_list(lexemes_filepath)\n",
    "mapping_dict = read_mapping(mapping_filepath)\n",
    "word_embed = gensim.models.KeyedVectors.load_word2vec_format(word2vec_filepath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word2vec_and_autoextend(word, synset_idx=0):\n",
    "    print('####    word2vec     #####')\n",
    "    for i, word_w2 in enumerate(word_embed.most_similar([word])):\n",
    "        print(f'{word_w2[0]}')\n",
    "\n",
    "    print('####    AutoExtend     #####')\n",
    "    most_similar_lexemes = get_most_similar_lexemes(word, lexemes_dict, lexemes_list, mapping_dict, rank_range=10, is_single=False, synset_idx=synset_idx)\n",
    "    for i, lexeme in enumerate(most_similar_lexemes[0]):\n",
    "        print(f'{lexeme[0][:-18]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.760945737361908),\n",
       " ('kitten', 0.7464985251426697),\n",
       " ('feline', 0.7326234579086304),\n",
       " ('beagle', 0.7150582671165466),\n",
       " ('puppy', 0.7075453400611877),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931973457336),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embed.most_similar(['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.760945737361908),\n",
       " ('kitten', 0.7464985251426697),\n",
       " ('feline', 0.7326234579086304),\n",
       " ('beagle', 0.7150582671165466),\n",
       " ('puppy', 0.7075453400611877),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931973457336),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embed.most_similar(['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験１ 単語実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 語義曖昧性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "suits\n",
      "lawsuit\n",
      "Suit\n",
      "Atta_chakki_delivery\n",
      "lawsuits\n",
      "countersuit\n",
      "polka_dot_clown\n",
      "His_showmanship_rhinestone\n",
      "complaint\n",
      "lawsuit_alleging\n",
      "####    AutoExtend     #####\n",
      "zoot_suit\n",
      "garment\n",
      "gabardine\n",
      "tailcoat\n",
      "tuxedo\n",
      "suit\n",
      "tux\n",
      "suit\n",
      "suit\n",
      "trousers\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_autoextend('suit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "chairs\n",
      "Chair\n",
      "chairperson\n",
      "chairwoman\n",
      "chairman\n",
      "Vice_Chair\n",
      "Co_Chair\n",
      "chairing\n",
      "Chairs\n",
      "cochair\n",
      "####    AutoExtend     #####\n",
      "seat\n",
      "sofa\n",
      "armchair\n",
      "stool\n",
      "recliner\n",
      "chaise_longue\n",
      "swivel_chair\n",
      "rocking_chair\n",
      "chaise\n",
      "lawn_chair\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_autoextend('chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "tables\n",
      "ConocoPhillips_BPAmerica\n",
      "Capitalized_Included\n",
      "tray\n",
      "dining_room\n",
      "banquette\n",
      "rapping_cappella\n",
      "sideboard\n",
      "linen_tablecloth\n",
      "Tables\n",
      "####    AutoExtend     #####\n",
      "worktable\n",
      "table\n",
      "bookcase\n",
      "chair\n",
      "room\n",
      "sideboard\n",
      "tray\n",
      "workbench\n",
      "furniture\n",
      "credenza\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_autoextend('table', synset_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上位下位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word2vec_and_hypo_hyper(word, synset_idx=0):\n",
    "    # print('####    word2vec     #####')\n",
    "    # for i, word_w2 in enumerate(word_embed.most_similar([word])):\n",
    "    #     print(f'{word_w2[0]}')\n",
    "    \n",
    "    # print('####    word2vec + 上位下位     #####')\n",
    "    # for i, word_w2 in enumerate(word_embed.most_similar([word])):\n",
    "    #     if i < 6:\n",
    "    #         print(f'{word_w2[0]}')\n",
    "    #     else:\n",
    "    #         break\n",
    "\n",
    "    print(f'{get_hypernum(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hypernum(get_hypernum(word, idx=synset_idx)[0])[0]}')\n",
    "    print(f'{get_hypernum(get_hypernum(get_hypernum(word, idx=synset_idx)[0])[0])[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garment\n",
      "clothing\n",
      "consumer_goods\n",
      "pinstripe\n",
      "single-breasted_suit\n",
      "business_suit\n",
      "slack_suit\n",
      "zoot_suit\n",
      "business_suit\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('suit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furniture\n",
      "furnishing\n",
      "accessory\n",
      "card_table\n",
      "worktable\n",
      "stand\n",
      "pedestal_table\n",
      "console_table\n",
      "tea_table\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('table', synset_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canine\n",
      "tooth\n",
      "bone\n",
      "pug\n",
      "lapdog\n",
      "lapdog\n",
      "mexican_hairless\n",
      "puppy\n",
      "basenji\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('dog', synset_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seat\n",
      "space\n",
      "attribute\n",
      "tablet-armed_chair\n",
      "barber_chair\n",
      "tablet-armed_chair\n",
      "straight_chair\n",
      "chair_of_state\n",
      "wheelchair\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('chair', synset_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ryogo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ryogo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from word_extractor.lexemes_vector import AutoextendExtractor\n",
    "from word_extractor.wordnet import get_hypernum, get_hyponym, get_not_similar_hyponym\n",
    "import gensim\n",
    "import random\n",
    "\n",
    "lexemes_filepath = './data/lexemes.txt'\n",
    "mapping_filepath = './data/mapping.txt'\n",
    "autoextend = AutoextendExtractor(lexemes_filepath, mapping_filepath)\n",
    "\n",
    "word2vec_filepath = './data/GoogleNews-vectors-negative300.bin'\n",
    "word_embed = gensim.models.KeyedVectors.load_word2vec_format(word2vec_filepath, binary=True)\n",
    "\n",
    "\n",
    "# 重複なし乱数\n",
    "def rand_ints_nodup(a, b, k):\n",
    "  ns = []\n",
    "  while len(ns) < k:\n",
    "    n = random.randint(a, b)\n",
    "    if not n in ns:\n",
    "      ns.append(n)\n",
    "  return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_by_similarity(word):\n",
    "    idx1, idx2, idx3  = rand_ints_nodup(0, 9, 3)\n",
    "    words = word_embed.most_similar([word])\n",
    "    results = word_embed.most_similar(positive=[words[idx1][0], words[idx2][0]],negative=[words[idx3][0]])\n",
    "    output_word = None\n",
    "    for result_word in results:\n",
    "        if (result_word != words[idx1][0]) and \\\n",
    "           (result_word != words[idx2][0]) and \\\n",
    "           (result_word != words[idx3][0]):\n",
    "           \n",
    "            output_word = result_word\n",
    "            break\n",
    "        \n",
    "    print(output_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_by_hypo_hyper(word, synset_idx=0):\n",
    "    sensekey_input = autoextend.get_senseky_from_word(word, synset_idx=0)\n",
    "    similar1 = autoextend.get_most_similar_lexemes(word, synset_idx=0)[1]\n",
    "    similar2 = autoextend.get_most_similar_lexemes(word, synset_idx=0)[1]\n",
    "    similar3 = autoextend.get_most_similar_lexemes(word, synset_idx=0)[1]\n",
    "    hyper = get_hypernum(word, idx=synset_idx)[1]\n",
    "    hyper_hyper = get_hypernum(get_hypernum(word, idx=synset_idx)[0])[1]\n",
    "    hypo1 = get_hyponym(word, idx=synset_idx)[1]\n",
    "    hypo2 = get_hyponym(word, idx=synset_idx)[1]\n",
    "    hypo3 = get_hyponym(word, idx=synset_idx)[1]\n",
    "\n",
    "    word_list = [sensekey_input, similar1, similar2, similar3, hyper, hyper_hyper, hypo1, hypo2, hypo3]\n",
    "    \n",
    "    for i in word_list:\n",
    "        if i is not None:\n",
    "            print(i)\n",
    "        else:\n",
    "            print('sensekey is None')\n",
    "\n",
    "    for _ in range(20):\n",
    "        idx1, idx2, idx3 = rand_ints_nodup(0, len(word_list)-1, 3)\n",
    "        autoextend.get_analogy_by_sensekey(word_list[idx1], word_list[idx2], word_list[idx3])\n",
    "\n",
    "    analogy_words = []\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(hyper, sensekey_input, similar1))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(hyper, sensekey_input, similar2))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(sensekey_input, hypo1, similar1))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(sensekey_input, hypo1, similar2))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(sensekey_input, hypo2, similar1))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(sensekey_input, hypo2, similar2))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(hyper_hyper, sensekey_input, similar1))\n",
    "    analogy_words.append(autoextend.get_analogy_by_sensekey(hyper_hyper, sensekey_input, similar2))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_by_hypo_hyper_other(word1, word2, synset_idx=0):\n",
    "    sensekey_input1 = autoextend.get_senseky_from_word(word1, synset_idx=0)\n",
    "    sensekey_input2 = autoextend.get_senseky_from_word(word2, synset_idx=0)\n",
    "    similar1 = autoextend.get_most_similar_lexemes(word1, synset_idx=0)[1]\n",
    "    similar2 = autoextend.get_most_similar_lexemes(word1, synset_idx=0)[1]\n",
    "    hyper = get_hypernum(word1, idx=synset_idx)[1]\n",
    "    hyper_hyper = get_hypernum(get_hypernum(word1, idx=synset_idx)[0])[1]\n",
    "    hyper_hyper_hyper = get_hypernum(get_hypernum(get_hypernum(word1, idx=synset_idx)[0])[0])[1]\n",
    "    hypo1 = get_hyponym(word1, idx=synset_idx)[1]\n",
    "    hypo2 = get_hyponym(word1, idx=synset_idx)[1]\n",
    "    \n",
    "    print(sensekey_input1, sensekey_input2 ,similar1, similar2, hyper, hyper_hyper, hypo1, hypo2)\n",
    "\n",
    "    autoextend.get_analogy_by_sensekey(hyper, sensekey_input1, sensekey_input2)\n",
    "    autoextend.get_analogy_by_sensekey(sensekey_input1, hypo1, sensekey_input2)\n",
    "    autoextend.get_analogy_by_sensekey(sensekey_input1, hypo2, sensekey_input2)\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper, sensekey_input1, sensekey_input2)\n",
    "\n",
    "    autoextend.get_analogy_by_sensekey(hyper, sensekey_input2, sensekey_input1)\n",
    "    autoextend.get_analogy_by_sensekey(sensekey_input2, hypo1, sensekey_input1)\n",
    "    autoextend.get_analogy_by_sensekey(sensekey_input2, hypo2, sensekey_input1)\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper, sensekey_input2, sensekey_input1)\n",
    "\n",
    "    print('hyper____')\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper_hyper, sensekey_input1, sensekey_input2)\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper_hyper, sensekey_input1, hypo1)\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper_hyper, sensekey_input2, hypo2)\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper_hyper, sensekey_input2, similar1)\n",
    "    autoextend.get_analogy_by_sensekey(hyper_hyper_hyper, sensekey_input2, similar2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table%1:14:00:: beauty%1:07:00:: table%1:06:01:: contents%1:10:00:: array%1:14:00:: arrangement%1:14:00:: contents%1:10:00:: actuarial_table%1:14:00::\n",
      "array - table + beauty = loveliness\n",
      "table - contents + beauty = glamor\n",
      "None\n",
      "arrangement - table + beauty = loveliness\n",
      "array - beauty + table = slender\n",
      "beauty - contents + table = glamor\n",
      "None\n",
      "arrangement - beauty + table = slender\n",
      "hyper____\n",
      "planning - table + beauty = loveliness\n",
      "planning - table + contents = contained\n",
      "None\n",
      "planning - beauty + table = slender\n",
      "planning - beauty + contents = table\n"
     ]
    }
   ],
   "source": [
    "analogy_by_hypo_hyper_other('table', 'beauty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dryer%1:06:00:: fun%1:04:00:: washing_machine%1:06:00:: stove%1:06:00:: appliance%1:06:00:: device%1:06:00:: clothes_dryer%1:06:00:: clothes_dryer%1:06:00::\n",
      "appliance - dryer + fun = durables\n",
      "dryer - clothes_dryer + fun = hair_dryer\n",
      "dryer - clothes_dryer + fun = hair_dryer\n",
      "device - dryer + fun = carpet\n",
      "appliance - fun + dryer = clothes_dryer\n",
      "fun - clothes_dryer + dryer = hair_dryer\n",
      "fun - clothes_dryer + dryer = hair_dryer\n",
      "device - fun + dryer = clothes_dryer\n",
      "hyper____\n",
      "instrumentality - dryer + fun = bag\n",
      "instrumentality - dryer + clothes_dryer = unlawfully\n",
      "instrumentality - fun + clothes_dryer = dryer\n",
      "instrumentality - fun + washing_machine = washer\n",
      "instrumentality - fun + stove = cooker\n"
     ]
    }
   ],
   "source": [
    "analogy_by_hypo_hyper_other('dryer', 'fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dryer%1:06:00:: strange%3:00:00:: water_heater%1:06:00:: microwave_oven%1:06:00:: appliance%1:06:00:: device%1:06:00:: clothes_dryer%1:06:00:: hand_blower%1:06:00::\n",
      "appliance - dryer + strange = durables\n",
      "dryer - clothes_dryer + strange = hair_dryer\n",
      "None\n",
      "device - dryer + strange = carpet\n",
      "appliance - strange + dryer = clothes_dryer\n",
      "strange - clothes_dryer + dryer = hair_dryer\n",
      "None\n",
      "device - strange + dryer = clothes_dryer\n",
      "hyper____\n",
      "instrumentality - dryer + strange = preserver\n",
      "instrumentality - dryer + clothes_dryer = unlawfully\n",
      "None\n",
      "instrumentality - strange + water_heater = stove\n",
      "instrumentality - strange + microwave_oven = microwave\n"
     ]
    }
   ],
   "source": [
    "analogy_by_hypo_hyper_other('dryer', 'strange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_analogy(word, synset_idx=0):\n",
    "    print('####    類似度の類推     #####')\n",
    "    for _ in range(10):\n",
    "        analogy_by_similarity(word)\n",
    "    \n",
    "    print('####    上位下位の類推     #####')\n",
    "    analogy_by_hypo_hyper(word, synset_idx=synset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    類似度の類推     #####\n",
      "('Complaint', 0.5637338161468506)\n",
      "('Mustard_pakki', 0.5386776924133301)\n",
      "('Al_Edhari', 0.49727824330329895)\n",
      "('pompadoured_hair', 0.6859167814254761)\n",
      "('lawsuit', 0.671337366104126)\n",
      "('pompadoured_hair', 0.6738342642784119)\n",
      "('lawsuits_alleging', 0.5511248707771301)\n",
      "('lawsuit', 0.5822794437408447)\n",
      "('alleging', 0.5666015148162842)\n",
      "('suit', 0.5710189938545227)\n",
      "####    上位下位の類推     #####\n",
      "suit%1:06:00::\n",
      "gabardine%1:06:03::\n",
      "zoot_suit%1:06:00::\n",
      "suit%1:06:01::\n",
      "garment%1:06:00::\n",
      "clothing%1:06:00::\n",
      "zoot_suit%1:06:00::\n",
      "slack_suit%1:06:00::\n",
      "double-breasted_suit%1:06:00::\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "suit - suit + gabardine = trousers\n",
      "None\n",
      "None\n",
      "None\n",
      "clothing - garment + suit = apparel\n",
      "zoot_suit - clothing + suit = porkpie_hat\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "suit - zoot_suit + garment = screamer\n",
      "gabardine - zoot_suit + garment = screamer\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "garment - suit + gabardine = trousers\n",
      "garment - suit + zoot_suit = jacket\n",
      "suit - zoot_suit + gabardine = screamer\n",
      "suit - zoot_suit + zoot_suit = garment\n",
      "None\n",
      "None\n",
      "clothing - suit + gabardine = clothes\n",
      "clothing - suit + zoot_suit = garment\n"
     ]
    }
   ],
   "source": [
    "compare_analogy('suit', synset_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    類似度の類推     #####\n",
      "('trays', 0.5813458561897278)\n",
      "('Ethan_Hession', 0.44691479206085205)\n",
      "('wine_goblet', 0.5873473286628723)\n",
      "('drill_sergeant_Perkovic', 0.47566887736320496)\n",
      "('reconciles_EBITDA', 0.44722437858581543)\n",
      "('dinning_room', 0.608176052570343)\n",
      "('reconciles_EBITDA', 0.44722437858581543)\n",
      "('drill_sergeant_Perkovic', 0.45419415831565857)\n",
      "('toasted_cashews', 0.4707796573638916)\n",
      "('tables', 0.5040887594223022)\n",
      "####    上位下位の類推     #####\n",
      "table%1:14:00::\n",
      "table%1:14:01::\n",
      "tray%1:06:00::\n",
      "drawer%1:18:01::\n",
      "furniture%1:06:00::\n",
      "furnishing%1:06:00::\n",
      "stand%1:06:04::\n",
      "conference_table%1:06:00::\n",
      "tea_table%1:06:00::\n",
      "None\n",
      "None\n",
      "table - drawer + tray = toast\n",
      "None\n",
      "None\n",
      "table - tray + table = draft\n",
      "None\n",
      "stand - drawer + furniture = cobblestone\n",
      "None\n",
      "None\n",
      "furniture - table + table = sideboard\n",
      "furniture - table + tray = turntable\n",
      "table - stand + table = club\n",
      "table - stand + tray = lazy_susan\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "compare_analogy('table', synset_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    類似度の類推     #####\n",
      "('Nancy_Pekarek_spokeswoman', 0.505904495716095)\n",
      "('stealth_removable', 0.580708384513855)\n",
      "('rider_Shinichi_Nakatomi', 0.5683863162994385)\n",
      "('No.##_Roush_Racing', 0.5009022951126099)\n",
      "('Rear_wheel', 0.5558632612228394)\n",
      "('footpeg', 0.609529435634613)\n",
      "####    上位下位の類推     #####\n",
      "wheel%1:06:00:: nosewheel%1:06:00:: paddlewheel%1:06:00:: machine%1:06:02:: device%1:06:00:: wagon_wheel%1:06:00:: daisy_print_wheel%1:06:00::\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "device - nosewheel + paddlewheel = sternwheeler\n",
      "None\n",
      "nosewheel - device + wheel = landing_gear\n",
      "None\n",
      "machine - wheel + nosewheel = tailplane\n",
      "machine - wheel + paddlewheel = sternwheeler\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "device - wheel + nosewheel = tailplane\n",
      "device - wheel + paddlewheel = sternwheeler\n"
     ]
    }
   ],
   "source": [
    "compare_analogy('wheel', synset_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    類似度の類推     #####\n",
      "('tire', 0.6417492032051086)\n",
      "('tire', 0.6120626330375671)\n",
      "('beadlock', 0.583296000957489)\n",
      "('Pirelli_tire', 0.5449734926223755)\n",
      "('Corteco', 0.5167824625968933)\n",
      "('tire', 0.5241057872772217)\n",
      "####    上位下位の類推     #####\n",
      "tire%1:06:00:: tubeless_tire%1:06:00:: tubeless_tire%1:06:00:: devolve%2:29:00:: delegating%1:04:00:: retire%2:37:00:: poop_out%2:29:00::\n",
      "None\n",
      "retire - tubeless_tire + tubeless_tire = withdraw\n",
      "None\n",
      "tubeless_tire - delegating + tubeless_tire = tubeless\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "retire - tubeless_tire + tire = connexion\n",
      "None\n",
      "None\n",
      "None\n",
      "tire - retire + tubeless_tire = pneumatic_tire\n",
      "tire - retire + tubeless_tire = pneumatic_tire\n",
      "None\n",
      "None\n",
      "delegating - tire + tubeless_tire = decentralizing\n",
      "delegating - tire + tubeless_tire = decentralizing\n"
     ]
    }
   ],
   "source": [
    "compare_analogy('tire', synset_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    類似度の類推     #####\n",
      "('Co_Chair', 0.6625238060951233)\n",
      "('Chairs', 0.6326252222061157)\n",
      "('chairperson', 0.6012726426124573)\n",
      "('chair', 0.6340569853782654)\n",
      "('cochairs', 0.5543997287750244)\n",
      "('Chairwoman', 0.7123143672943115)\n",
      "('Co_Chair', 0.6384541392326355)\n",
      "('Chair_Elect', 0.60837721824646)\n",
      "('Chairperson', 0.6735294461250305)\n",
      "('chaired', 0.555755078792572)\n",
      "####    上位下位の類推     #####\n",
      "chair%1:06:00:: stool%1:06:00:: chaise_longue%1:06:00:: seat%1:06:01:: space%1:15:00:: folding_chair%1:06:00:: straight_chair%1:06:00::\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "chair - barber_chair + chaise_longue = chaise\n",
      "None\n",
      "chaise_longue - armchair + space = chaise\n",
      "space - folding_chair + chair = livingroom\n",
      "None\n",
      "None\n",
      "seat - chair + stool = footstool\n",
      "seat - chair + chaise_longue = chaise\n",
      "chair - folding_chair + stool = milking_stool\n",
      "chair - folding_chair + chaise_longue = chaise\n",
      "None\n",
      "None\n",
      "space - chair + stool = harborage\n",
      "space - chair + chaise_longue = chaise\n"
     ]
    }
   ],
   "source": [
    "compare_analogy('chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    類似度の類推     #####\n",
      "('pompadoured_hair', 0.6018907427787781)\n",
      "('alleging', 0.580407977104187)\n",
      "('Wearing_beige', 0.5911272764205933)\n",
      "('Al_Edhari', 0.47900304198265076)\n",
      "('lawsuit', 0.5446116328239441)\n",
      "('sported_bushy_reddish_sideburns', 0.5645790100097656)\n",
      "####    上位下位の類推     #####\n",
      "suit%1:06:00:: tuxedo%1:06:00:: trousers%1:06:00:: garment%1:06:00:: clothing%1:06:00:: slack_suit%1:06:00:: slack_suit%1:06:00::\n",
      "None\n",
      "None\n",
      "None\n",
      "garment - clothing + trousers = jacket\n",
      "None\n",
      "None\n",
      "clothing - garment + tuxedo = tux\n",
      "trousers - garment + clothing = clothes\n",
      "None\n",
      "None\n",
      "garment - suit + tuxedo = tux\n",
      "garment - suit + trousers = skirt\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "clothing - suit + tuxedo = tux\n",
      "clothing - suit + trousers = clothes\n"
     ]
    }
   ],
   "source": [
    "compare_analogy('suit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauty%1:07:00:: prettiness%1:07:00:: loveliness%1:07:00:: appearance%1:07:00:: quality%1:07:00:: pulchritude%1:07:00:: pulchritude%1:07:00::\n",
      "appearance beauty beauty\n",
      "['slender', 'art', 'light', 'light', 'light', 'clear', 'light', 'mercantilism', 'shape', 'mug']\n",
      "appearance - beauty + beauty = slender\n",
      "appearance beauty beauty\n",
      "['slender', 'art', 'light', 'light', 'light', 'clear', 'light', 'mercantilism', 'shape', 'mug']\n",
      "appearance - beauty + beauty = slender\n",
      "beauty pulchritude pulchritude\n",
      "['forward', 'blow', 'blow', 'have', 'turn', 'sheer', 'fresh', 'blow', 'inflation', 'heavy']\n",
      "beauty - pulchritude + pulchritude = forward\n",
      "beauty pulchritude pulchritude\n",
      "['forward', 'blow', 'blow', 'have', 'turn', 'sheer', 'fresh', 'blow', 'inflation', 'heavy']\n",
      "beauty - pulchritude + pulchritude = forward\n",
      "beauty pulchritude pulchritude\n",
      "['forward', 'blow', 'blow', 'have', 'turn', 'sheer', 'fresh', 'blow', 'inflation', 'heavy']\n",
      "beauty - pulchritude + pulchritude = forward\n",
      "beauty pulchritude pulchritude\n",
      "['forward', 'blow', 'blow', 'have', 'turn', 'sheer', 'fresh', 'blow', 'inflation', 'heavy']\n",
      "beauty - pulchritude + pulchritude = forward\n",
      "quality beauty beauty\n",
      "['slender', 'supple', 'table', 'stylist', 'art', 'mercantilism', 'chassis', 'shape', 'hairy', 'file']\n",
      "quality - beauty + beauty = slender\n",
      "quality beauty beauty\n",
      "['slender', 'supple', 'table', 'stylist', 'art', 'mercantilism', 'chassis', 'shape', 'hairy', 'file']\n",
      "quality - beauty + beauty = slender\n"
     ]
    }
   ],
   "source": [
    "analogy_by_hypo_hyper_other('beauty', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beauty%1:07:00:: loveliness%1:07:00:: comeliness%1:07:00:: appearance%1:07:00:: quality%1:07:00:: picturesqueness%1:07:00:: handsomeness%1:07:00::\n",
      "appearance beauty beauty\n",
      "['chair', 'swivel_chair', 'armchair', 'overstuffed_chair', 'highchair', 'recliner', 'rocking_chair', 'wheelchair', 'reclining_chair', 'seat']\n",
      "appearance - beauty + beauty = chair\n",
      "appearance beauty beauty\n",
      "['chair', 'swivel_chair', 'armchair', 'overstuffed_chair', 'highchair', 'recliner', 'rocking_chair', 'wheelchair', 'reclining_chair', 'seat']\n",
      "appearance - beauty + beauty = chair\n",
      "beauty picturesqueness picturesqueness\n",
      "None\n",
      "beauty picturesqueness picturesqueness\n",
      "None\n",
      "beauty handsomeness handsomeness\n",
      "['heavy', 'chair', 'quilt', 'bookfair', 'crochet', 'mat', 'peoria', 'picnic', 'heavy', 'fair']\n",
      "beauty - handsomeness + handsomeness = heavy\n",
      "beauty handsomeness handsomeness\n",
      "['heavy', 'chair', 'quilt', 'bookfair', 'crochet', 'mat', 'peoria', 'picnic', 'heavy', 'fair']\n",
      "beauty - handsomeness + handsomeness = heavy\n",
      "quality beauty beauty\n",
      "['chair', 'swivel_chair', 'armchair', 'recliner', 'stool', 'rocking_chair', 'highchair', 'footrest', 'overstuffed_chair', 'seat']\n",
      "quality - beauty + beauty = chair\n",
      "quality beauty beauty\n",
      "['chair', 'swivel_chair', 'armchair', 'recliner', 'stool', 'rocking_chair', 'highchair', 'footrest', 'overstuffed_chair', 'seat']\n",
      "quality - beauty + beauty = chair\n"
     ]
    }
   ],
   "source": [
    "analogy_by_hypo_hyper_other('beauty', 'chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compare_analogy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xl/d7b96jtx11935vw0d7m2sj0r0000gn/T/ipykernel_15290/1437507374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_analogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vase'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'compare_analogy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e8a326d5694a9d8c74f302fee0d42ce12ff4040a9c13a30d2f66dd02efa87f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('keyword_generation-2Tlv01LJ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
