{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryogo/.local/share/virtualenvs/keyword_generation-2Tlv01LJ/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to /Users/ryogo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ryogo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from word_extractor.lexemes_vector import get_most_similar_lexemes, get_verctor_from_word, get_verctor_from_sense_key, get_word_from_vector\n",
    "from word_extractor.wordnet import get_hypernum, get_hyponym, get_not_similar_hyponym\n",
    "from utils.dataloader import read_lexemes_dict_and_list, read_mapping\n",
    "\n",
    "input_words = ['car', 'toughness', 'velocity', 'tire']\n",
    "synset_filepath = './data/synsets.txt'\n",
    "lexemes_filepath = './data/lexemes.txt'\n",
    "mapping_filepath = './data/mapping.txt'\n",
    "word2vec_filepath = './data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "lexemes_dict, lexemes_list = read_lexemes_dict_and_list(lexemes_filepath)\n",
    "mapping_dict = read_mapping(mapping_filepath)\n",
    "word_embed = gensim.models.KeyedVectors.load_word2vec_format(word2vec_filepath, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word2vec_and_autoextend(word, synset_idx=0):\n",
    "    print('####    word2vec     #####')\n",
    "    for i, word_w2 in enumerate(word_embed.most_similar([word])):\n",
    "        print(f'{word_w2[0]}')\n",
    "\n",
    "    print('####    AutoExtend     #####')\n",
    "    most_similar_lexemes = get_most_similar_lexemes(word, lexemes_dict, lexemes_list, mapping_dict, rank_range=10, is_single=False, synset_idx=synset_idx)\n",
    "    for i, lexeme in enumerate(most_similar_lexemes[0]):\n",
    "        print(f'{lexeme[0][:-18]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験１ 単語実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 語義曖昧性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "suits\n",
      "lawsuit\n",
      "Suit\n",
      "Atta_chakki_delivery\n",
      "lawsuits\n",
      "countersuit\n",
      "polka_dot_clown\n",
      "His_showmanship_rhinestone\n",
      "complaint\n",
      "lawsuit_alleging\n",
      "####    AutoExtend     #####\n",
      "zoot_suit\n",
      "garment\n",
      "gabardine\n",
      "tailcoat\n",
      "tuxedo\n",
      "suit\n",
      "tux\n",
      "suit\n",
      "suit\n",
      "trousers\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_autoextend('suit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "chairs\n",
      "Chair\n",
      "chairperson\n",
      "chairwoman\n",
      "chairman\n",
      "Vice_Chair\n",
      "Co_Chair\n",
      "chairing\n",
      "Chairs\n",
      "cochair\n",
      "####    AutoExtend     #####\n",
      "seat\n",
      "sofa\n",
      "armchair\n",
      "stool\n",
      "recliner\n",
      "chaise_longue\n",
      "swivel_chair\n",
      "rocking_chair\n",
      "chaise\n",
      "lawn_chair\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_autoextend('chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "tables\n",
      "ConocoPhillips_BPAmerica\n",
      "Capitalized_Included\n",
      "tray\n",
      "dining_room\n",
      "banquette\n",
      "rapping_cappella\n",
      "sideboard\n",
      "linen_tablecloth\n",
      "Tables\n",
      "####    AutoExtend     #####\n",
      "worktable\n",
      "table\n",
      "bookcase\n",
      "chair\n",
      "room\n",
      "sideboard\n",
      "tray\n",
      "workbench\n",
      "furniture\n",
      "credenza\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_autoextend('table', synset_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上位下位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word2vec_and_hypo_hyper(word, synset_idx=0):\n",
    "    print('####    word2vec     #####')\n",
    "    for i, word_w2 in enumerate(word_embed.most_similar([word])):\n",
    "        print(f'{word_w2[0]}')\n",
    "    \n",
    "    print('####    word2vec + 上位下位     #####')\n",
    "    for i, word_w2 in enumerate(word_embed.most_similar([word])):\n",
    "        if i < 6:\n",
    "            print(f'{word_w2[0]}')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f'{get_hypernum(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hypernum(get_hypernum(word, idx=synset_idx)[0])[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    print(f'{get_hyponym(word, idx=synset_idx)[0]}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "suits\n",
      "lawsuit\n",
      "Suit\n",
      "Atta_chakki_delivery\n",
      "lawsuits\n",
      "countersuit\n",
      "polka_dot_clown\n",
      "His_showmanship_rhinestone\n",
      "complaint\n",
      "lawsuit_alleging\n",
      "####    word2vec + 上位下位     #####\n",
      "suits\n",
      "lawsuit\n",
      "Suit\n",
      "Atta_chakki_delivery\n",
      "lawsuits\n",
      "countersuit\n",
      "garment\n",
      "clothing\n",
      "slack_suit\n",
      "double-breasted_suit\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('suit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "tables\n",
      "ConocoPhillips_BPAmerica\n",
      "Capitalized_Included\n",
      "tray\n",
      "dining_room\n",
      "banquette\n",
      "rapping_cappella\n",
      "sideboard\n",
      "linen_tablecloth\n",
      "Tables\n",
      "####    word2vec + 上位下位     #####\n",
      "tables\n",
      "ConocoPhillips_BPAmerica\n",
      "Capitalized_Included\n",
      "tray\n",
      "dining_room\n",
      "banquette\n",
      "furniture\n",
      "furnishing\n",
      "card_table\n",
      "desk\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('table', synset_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "dogs\n",
      "puppy\n",
      "pit_bull\n",
      "pooch\n",
      "cat\n",
      "golden_retriever\n",
      "German_shepherd\n",
      "Rottweiler\n",
      "beagle\n",
      "pup\n",
      "####    word2vec + 上位下位     #####\n",
      "dogs\n",
      "puppy\n",
      "pit_bull\n",
      "pooch\n",
      "cat\n",
      "golden_retriever\n",
      "canine\n",
      "tooth\n",
      "spitz\n",
      "toy_dog\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('dog', synset_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####    word2vec     #####\n",
      "chairs\n",
      "Chair\n",
      "chairperson\n",
      "chairwoman\n",
      "chairman\n",
      "Vice_Chair\n",
      "Co_Chair\n",
      "chairing\n",
      "Chairs\n",
      "cochair\n",
      "####    word2vec + 上位下位     #####\n",
      "chairs\n",
      "Chair\n",
      "chairperson\n",
      "chairwoman\n",
      "chairman\n",
      "Vice_Chair\n",
      "seat\n",
      "space\n",
      "armchair\n",
      "rocking_chair\n"
     ]
    }
   ],
   "source": [
    "show_word2vec_and_hypo_hyper('chair', synset_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類推"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e8a326d5694a9d8c74f302fee0d42ce12ff4040a9c13a30d2f66dd02efa87f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('keyword_generation-2Tlv01LJ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
